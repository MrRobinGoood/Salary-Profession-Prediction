{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "751bc0f6-2298-4112-9e6f-df1627cafbf3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-08T03:10:09.871845Z",
     "iopub.status.busy": "2024-09-08T03:10:09.870518Z",
     "iopub.status.idle": "2024-09-08T03:10:09.905375Z",
     "shell.execute_reply": "2024-09-08T03:10:09.904181Z",
     "shell.execute_reply.started": "2024-09-08T03:10:09.871794Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "print('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd7a0c5e-164c-48e6-b228-2dffc3460e41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-08T01:19:48.028074Z",
     "iopub.status.busy": "2024-09-08T01:19:48.026629Z",
     "iopub.status.idle": "2024-09-08T01:19:52.343247Z",
     "shell.execute_reply": "2024-09-08T01:19:52.341838Z",
     "shell.execute_reply.started": "2024-09-08T01:19:48.028031Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorflow-gpu in /home/jupyter/.local/lib/python3.10/site-packages (2.11.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-gpu) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-gpu) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /home/jupyter/.local/lib/python3.10/site-packages (from tensorflow-gpu) (24.3.25)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-gpu) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-gpu) (0.2.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow-gpu) (1.56.2)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/jupyter/.local/lib/python3.10/site-packages (from tensorflow-gpu) (3.11.0)\n",
      "Requirement already satisfied: keras<2.12,>=2.11.0 in /home/jupyter/.local/lib/python3.10/site-packages (from tensorflow-gpu) (2.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-gpu) (16.0.6)\n",
      "Requirement already satisfied: numpy>=1.20 in /home/jupyter/.local/lib/python3.10/site-packages (from tensorflow-gpu) (1.26.4)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow-gpu) (3.3.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-gpu) (23.1)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /home/jupyter/.local/lib/python3.10/site-packages (from tensorflow-gpu) (3.19.6)\n",
      "Requirement already satisfied: setuptools in /kernel/lib/python3.10/site-packages (from tensorflow-gpu) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow-gpu) (1.16.0)\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in /home/jupyter/.local/lib/python3.10/site-packages (from tensorflow-gpu) (2.11.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /home/jupyter/.local/lib/python3.10/site-packages (from tensorflow-gpu) (2.11.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-gpu) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow-gpu) (4.7.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-gpu) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-gpu) (0.32.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow-gpu) (0.41.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow-gpu) (2.17.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/jupyter/.local/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow-gpu) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow-gpu) (3.4.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow-gpu) (2.27.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/jupyter/.local/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow-gpu) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/jupyter/.local/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow-gpu) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow-gpu) (2.3.6)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-gpu) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-gpu) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-gpu) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-gpu) (1.3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-gpu) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-gpu) (2023.7.22)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-gpu) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-gpu) (3.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow-gpu) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-gpu) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-gpu) (3.2.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05237eac-4f4e-4456-9c68-8369ce87d464",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-08T01:20:26.685947Z",
     "iopub.status.busy": "2024-09-08T01:20:26.684474Z",
     "iopub.status.idle": "2024-09-08T01:20:29.514833Z",
     "shell.execute_reply": "2024-09-08T01:20:29.513597Z",
     "shell.execute_reply.started": "2024-09-08T01:20:26.685898Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: keras in /home/jupyter/.local/lib/python3.10/site-packages (2.11.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/jupyter/.local/lib/python3.10/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "pip install keras scikit-learn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3956e954-a60a-42e5-8feb-190c21b77e17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-08T03:10:37.979900Z",
     "iopub.status.busy": "2024-09-08T03:10:37.978316Z",
     "iopub.status.idle": "2024-09-08T03:10:56.092072Z",
     "shell.execute_reply": "2024-09-08T03:10:56.090829Z",
     "shell.execute_reply.started": "2024-09-08T03:10:37.979846Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-08 03:10:38.016681: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-08 03:10:43.863041: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda/lib64:/usr/local/cuda/lib64\n",
      "2024-09-08 03:10:43.863605: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda/lib64:/usr/local/cuda/lib64\n",
      "2024-09-08 03:10:43.863626: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b7503a9-4f3c-42b4-902a-ec5aa8351a60",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-08T03:11:10.676806Z",
     "iopub.status.busy": "2024-09-08T03:11:10.675332Z",
     "iopub.status.idle": "2024-09-08T03:11:10.706793Z",
     "shell.execute_reply": "2024-09-08T03:11:10.705577Z",
     "shell.execute_reply.started": "2024-09-08T03:11:10.676769Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Замените на номер вашей GPU (если более одной)\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    for device in physical_devices:\n",
    "        tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9069f42a-c0ce-4400-b4d7-de6b1f9ca4ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-08T03:11:37.091123Z",
     "iopub.status.busy": "2024-09-08T03:11:37.089629Z",
     "iopub.status.idle": "2024-09-08T03:11:39.509388Z",
     "shell.execute_reply": "2024-09-08T03:11:39.508150Z",
     "shell.execute_reply.started": "2024-09-08T03:11:37.091065Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "368e265e-9515-4e9d-a8f7-7a9cb6c1dec7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-08T03:11:42.841181Z",
     "iopub.status.busy": "2024-09-08T03:11:42.839913Z",
     "iopub.status.idle": "2024-09-08T03:12:12.785009Z",
     "shell.execute_reply": "2024-09-08T03:12:12.783535Z",
     "shell.execute_reply.started": "2024-09-08T03:11:42.841140Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_TRAIN_RES_1 = pd.read_csv('/home/jupyter/datasphere/project/TRAIN_RES_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb040af4-47be-41af-bfc4-07a6636bb39b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-08T03:22:05.485699Z",
     "iopub.status.busy": "2024-09-08T03:22:05.484295Z",
     "iopub.status.idle": "2024-09-08T03:22:05.527747Z",
     "shell.execute_reply": "2024-09-08T03:22:05.526347Z",
     "shell.execute_reply.started": "2024-09-08T03:22:05.485646Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df_TRAIN_RES_1\n",
    "df = df.iloc[0:200000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8c4ccdc-ab25-4af0-a7cb-62b63f68f199",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-08T03:12:36.778011Z",
     "iopub.status.busy": "2024-09-08T03:12:36.776872Z",
     "iopub.status.idle": "2024-09-08T03:17:29.905360Z",
     "shell.execute_reply": "2024-09-08T03:17:29.904083Z",
     "shell.execute_reply.started": "2024-09-08T03:12:36.777974Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Прошло кодирование целевых переменных\n",
      "Количество уникальных значаний в y_encoded: 638\n",
      "Всего уникальных значаний в y_encoded: 638\n",
      "Прошло удаление классов с одним представителем. Оставшиеся уникальные классы: 483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:29<00:00, 1719.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Прошло создание текстовых данных\n",
      "Прошло разделение данных\n",
      "Создано словарь label_mapping с 483 элементами.\n",
      "Запуск обучения модели...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Обучение модели: 398760it [04:14, 1564.89it/s, Val Loss=1.9231, Val Acc=0.5843]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Прошло обучение модели\n",
      "312/312 [==============================] - 2s 4ms/step\n",
      "Прошло генерация предсказаний\n",
      "F1-score (micro): 0.584\n",
      "Модель успешно сохранена в файл /home/jupyter/datasphere/project/model.h5\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "\n",
    "# Кодирование целевых переменных\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(df['job_title'])\n",
    "print(\"Прошло кодирование целевых переменных\")\n",
    "\n",
    "# Проверяем количество уникальных значаний\n",
    "print(f\"Количество уникальных значаний в y_encoded: {len(np.unique(y_encoded))}\")\n",
    "print(f\"Всего уникальных значаний в y_encoded: {len(set(y_encoded))}\")\n",
    "\n",
    "# Проверяем количество представителей каждого класса\n",
    "class_counts = np.bincount(y_encoded)\n",
    "\n",
    "# Создаем новый массив y_filtered, удаляя классы с одним представителем\n",
    "y_filtered = [x for x in y_encoded if class_counts[x] > 1]\n",
    "print(f\"Прошло удаление классов с одним представителем. Оставшиеся уникальные классы: {len(set(y_filtered))}\")\n",
    "\n",
    "# Создаем новые текстовые данные, учитывая только уникальные классы\n",
    "text_data = []\n",
    "for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    job_title = row['job_title']\n",
    "    class_id = le.transform([job_title])[0]\n",
    "    if class_id in y_filtered:\n",
    "        combined_text = f\"{row['achievements_modified']} {row['demands']}\"\n",
    "        text_data.append(combined_text)\n",
    "print(\"Прошло создание текстовых данных\")\n",
    "\n",
    "# Разделяем данные\n",
    "train_text, val_text, train_labels, val_labels = train_test_split(\n",
    "    text_data,\n",
    "    y_filtered,\n",
    "    random_state=42,\n",
    "    test_size=0.2,\n",
    "    stratify=y_filtered\n",
    ")\n",
    "print(\"Прошло разделение данных\")\n",
    "\n",
    "# Преобразуем train_labels и val_labels в NumPy массивы\n",
    "train_labels = np.array(train_labels)\n",
    "val_labels = np.array(val_labels)\n",
    "\n",
    "# Создаем токенизатор\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(train_text)\n",
    "\n",
    "# Преобразуем тексты в последовательности слов\n",
    "train_sequences = tokenizer.texts_to_sequences(train_text)\n",
    "val_sequences = tokenizer.texts_to_sequences(val_text)\n",
    "\n",
    "# Добавляем нулевые значения для коротких последовательностей\n",
    "pad_length = 200\n",
    "padded_train = pad_sequences(train_sequences, maxlen=pad_length)\n",
    "padded_val = pad_sequences(val_sequences, maxlen=pad_length)\n",
    "\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "# Создаем модель\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=5000, output_dim=128),\n",
    "    LSTM(units=64, dropout=0.2),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(len(set(y_filtered)), activation='softmax')\n",
    "])\n",
    "\n",
    "# Компилируем модель\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Функция для преобразования меток\n",
    "def transform_labels(labels):\n",
    "    return np.array([min(max(label, 0), len(label_mapping) - 1) for label in labels])\n",
    "\n",
    "# Создаем словарь для замены меток\n",
    "label_mapping = {k: v for k, v in zip(range(len(set(y_filtered))), set(y_filtered))}\n",
    "print(f\"Создано словарь label_mapping с {len(label_mapping)} элементами.\")\n",
    "\n",
    "# Обучаем модель\n",
    "print(\"Запуск обучения модели...\")\n",
    "with tqdm(total=train_labels.shape[0], desc=\"Обучение модели\") as pbar:\n",
    "    for epoch in range(10):\n",
    "        transformed_train_labels = transform_labels(train_labels)\n",
    "        transformed_val_labels = transform_labels(val_labels)\n",
    "        history = model.fit(padded_train, transformed_train_labels, epochs=1, \n",
    "                            validation_data=(padded_val, transformed_val_labels), \n",
    "                            verbose=0)\n",
    "        val_loss, val_accuracy = model.evaluate(padded_val, transformed_val_labels, verbose=0)\n",
    "        pbar.set_postfix({\"Val Loss\": f\"{val_loss:.4f}\", \"Val Acc\": f\"{val_accuracy:.4f}\"})\n",
    "        pbar.update(train_labels.shape[0])\n",
    "    \n",
    "    pbar.close()\n",
    "\n",
    "print(\"Прошло обучение модели\")\n",
    "\n",
    "# Генерация предсказаний\n",
    "predictions = model.predict(padded_val)\n",
    "predicted_classes = predictions.argmax(axis=-1)\n",
    "\n",
    "print(\"Прошло генерация предсказаний\")\n",
    "\n",
    "# Расчет F1-score\n",
    "f1_micro = f1_score(transformed_val_labels, predicted_classes, average='micro')\n",
    "\n",
    "print(f\"F1-score (micro): {f1_micro:.3f}\")\n",
    "\n",
    "model_path = '/home/jupyter/datasphere/project/model_new.h5'\n",
    "model.save(model_path)\n",
    "print(f\"Модель успешно сохранена в файл {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5832364-1b1e-4968-b360-6f36eafdcafb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-08T04:06:42.729861Z",
     "iopub.status.busy": "2024-09-08T04:06:42.728688Z",
     "iopub.status.idle": "2024-09-08T04:06:44.398392Z",
     "shell.execute_reply": "2024-09-08T04:06:44.397154Z",
     "shell.execute_reply.started": "2024-09-08T04:06:42.729819Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель успешно загружена из файла /home/jupyter/datasphere/project/model.h5\n",
      "62/62 [==============================] - 1s 4ms/step\n",
      "Предсказания успешно сохранены в файл /home/jupyter/datasphere/project/output_file2.csv\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Путь к загружаемой модели\n",
    "model_path = '/home/jupyter/datasphere/project/model.h5'\n",
    "\n",
    "# Загрузка модели\n",
    "model = load_model(model_path)\n",
    "print(f\"Модель успешно загружена из файла {model_path}\")\n",
    "\n",
    "# Загрузка нового файла с данными (например, в формате CSV)\n",
    "new_data_path = '/home/jupyter/datasphere/project/TEST_RES.csv'  # Укажите путь к вашему новому файлу\n",
    "new_df = pd.read_csv(new_data_path)\n",
    "\n",
    "# Предполагается, что в новом файле есть столбцы 'achievements_modified' и 'demands'\n",
    "# Создание текстовых данных для предсказания\n",
    "new_text_data = []\n",
    "for _, row in new_df.iterrows():\n",
    "    combined_text = f\"{row['achievements_modified']} {row['demands']}\"\n",
    "    new_text_data.append(combined_text)\n",
    "\n",
    "# Токенизация новых текстов\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(new_text_data)  # Обучить токенизатор на новых данных\n",
    "\n",
    "# Преобразование текстов в последовательности\n",
    "new_sequences = tokenizer.texts_to_sequences(new_text_data)\n",
    "\n",
    "# Дополнение последовательностей до одинаковой длины\n",
    "pad_length = 200  # Должно совпадать с длиной, использованной при обучении\n",
    "padded_new = pad_sequences(new_sequences, maxlen=pad_length)\n",
    "\n",
    "# Применение модели к новым данным\n",
    "predictions = model.predict(padded_new)\n",
    "predicted_classes = np.argmax(predictions, axis=-1)\n",
    "\n",
    "# Если хотите преобразовать предсказанные классы обратно в названия\n",
    "predicted_job_titles = le.inverse_transform(predicted_classes)\n",
    "\n",
    "# Добавление предсказаний в DataFrame для вывода\n",
    "new_df['job_title'] = predicted_job_titles\n",
    "\n",
    "# Сохранение результатов в новый файл\n",
    "output_path = '/home/jupyter/datasphere/project/output_file2.csv'  # Укажите путь к выходному файлу\n",
    "new_df.to_csv(output_path, index=False)\n",
    "print(f\"Предсказания успешно сохранены в файл {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5451e269-4e3c-4ca1-ac19-06f5602a06f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
